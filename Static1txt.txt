- The historical stock price data for Apple (AAPL) is loaded from a CSV file.
The date column is set as the index for easier time series analysis.
Missing values are filled using a forward-fill method, which carries the last known value forward to fill in any gaps.
- Creating Lagged Features
Lagged features are created from the 'Close' price to include information about past prices. For example:
Lag1 refers to the closing price of the previous day.
Lag2 refers to the closing price from two days ago.
Lag3 refers to the closing price from three days ago.
These lagged features help the model learn patterns from recent price movements.
- Calculating a Moving Average
A simple moving average (SMA) is calculated over a short window (just 1 day in this case). This is typically used to smooth out short-term fluctuations and identify trends.
The SMA serves as an additional feature for the model, providing a sense of price trends.
- Defining the Target Variable
The target variable is set as the next day's closing price, which is achieved by shifting the 'Close' price forward by one day.
This way, today’s price and features are paired with tomorrow’s closing price as the target.
- Handling Missing Values
After shifting and calculating lagged features, some rows may contain NaN values.
These rows are dropped to ensure the dataset is complete and ready for training.
- Setting Up Features and Target Variables
A set of features (e.g., current price, lagged prices, and SMA) is selected for training the model.
The target variable is the next day's closing price.
- Scaling the Data
The features are standardized using a scaler, which transforms them to have a mean of 0 and a standard deviation of 1.
This is a common practice in machine learning to ensure that all features are on the same scale, which can help improve the model's performance.
- Splitting Data into Training and Testing Sets
The scaled data is split into training and testing sets, with 80% used for training and 20% for testing.
This allows the model to learn patterns from the training set and then be evaluated on how well it generalizes to unseen data in the testing set.
- Building and Training the Neural Network
A neural network is defined with three layers:
Two hidden layers with 64 and 32 neurons, each using the ReLU activation function to introduce non-linearity.
An output layer with a single neuron to predict the next day's closing price.
The model is compiled using the Adam optimizer, which adjusts the learning rate during training.
It minimizes the mean squared error (MSE), which measures how close the model's predictions are to the actual values.
The model is trained for a set number of epochs (iterations over the training data) with a portion of data held back for validation during training.
- Generating Predictions
After training, the model makes predictions for the entire dataset.
These predictions are added as a new column in the dataset for comparison with actual closing prices.
- Forecasting the Next Day’s Price
The most recent data point (latest known prices) is extracted and scaled to match the input format.
This scaled data is used as input to the model to forecast the next day’s closing price.
The prediction is then unscaled to reflect the actual price level.